%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% University Assignment Title Page 
% LaTeX Template
% Version 1.0 (27/12/12)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original author:
% WikiBooks (http://en.wikibooks.org/wiki/LaTeX/Title_Creation)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\title{Title page with logo}
%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[12pt]{article}
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage{natbib}
\usepackage{color}
\usepackage[explicit]{titlesec}
\usepackage{url}
\usepackage{graphicx}
\usepackage[section]{placeins}


\begin{document}

\begin{titlepage}

\newcommand{\HRule}{\rule{\linewidth}{0.5mm}} % Defines a new command for the horizontal lines, change thickness here

\center % Center everything on the page
 
%----------------------------------------------------------------------------------------
%	HEADING SECTIONS
%----------------------------------------------------------------------------------------

\textsc{\LARGE University of St Andrews}\\[1.5cm] % Name of your university/college
\textsc{\Large CS5014 }\\[0.5cm] % Major heading such as course name
\textsc{\large Practical 1}\\[0.5cm] % Minor heading such as course title

%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\HRule \\[0.4cm]
{ \huge \bfseries Machine Learning}\\[0.4cm] % Title of your document
\HRule \\[1.5cm]
 
%----------------------------------------------------------------------------------------
%	AUTHOR SECTION
%----------------------------------------------------------------------------------------


\Large \emph{Author:}\\
 \textsc{150008022}\\[3cm] % Your name

%----------------------------------------------------------------------------------------
%	DATE SECTION
%----------------------------------------------------------------------------------------

{\large \today}\\[2cm] % Date, change the \today to a set date if you want to be precise

%----------------------------------------------------------------------------------------
%	LOGO SECTION
%---------------------------------------------------------------------------------------

\includegraphics[width = 3.1cm]{images/standrewslogo.png}
 
%----------------------------------------------------------------------------------------

\vfill % Fill the rest of the page with whitespace

\end{titlepage}

\pagenumbering{gobble}

\part*{Goal}

The goal of this practical was to cleanse and process real world data in order to produce a regression model, and evaluate its performance.

\tableofcontents

\pagebreak

\pagenumbering{arabic}
\setcounter{page}{1}

\part{Loading and Cleaning the Data}

Numpy was used to load the csv file. The header row was skipped, and to ensure there were no missing values, the invalid raise flag was also used when parsing the data. This would raise an exception if any rows were found to be missing data. 

No preproccessing had occurred on the data set according to the original paper, and so the data could be used as given.

The input and output columns were separated into two variables x and y, in accordance with the notation used in lectures.

Next, the testing set was isolated from the available data. Simple random sampling (SRS) and stratified sampling were considered for performing the data splitting. SRS is intuitive, but for less uniformly distributed data sets, it can lead to subsets that poorly represent the input data, and therefore suffer from sampling bias \cite{reitermanova_2010}. 

For these reasons, stratified random sampling was used with a 80\%-20\% split.  To ensure our training and testing sets were representative of our data, the output variables were used to categorise the data into strata. Since the two appeared to have a linear relationship as shown in figure \ref{fig:y1vsy2}, we could assume that using one column for categorising our data would produce an equal distribution of the values in both columns.

The 586 possible values of Y1 were split into 50 bins using \url{numpy.digitize}, and then passed to \url{train_test_split} as the \url{stratify} argument. 

\part{Analysing and Visualising the Data}

Firstly, a histogram of each of the input variables and outputs were plotted in order to visualise the distribution of the values (Figures \ref{fig:xdist} and \ref{fig:ydist}). When comparing to the histograms from the given paper \cite{tsanas_xifara_2012}, most of the plots matched. Any differences were identified to be caused by 10 bins always being used (the default if not specified by numpy.hist), and the paper would sometimes use more. However, it was still clear that none of the variables had a gaussian distribution. The output variables also appeared tail heavy, but the inputs did not.

\begin{figure}[!ht]
\centering
\includegraphics[width=0.8 \linewidth]{images/XDist}
\caption{Distribution of input variables}
\label{fig:xdist}
\end{figure}

\begin{figure}[!ht]
\centering
\includegraphics[width=0.8 \linewidth]{images/YDist}
\caption{Distribution of output variables}
\label{fig:ydist}
\end{figure}

\FloatBarrier

In order to identify which variables had the strongest relationships with the outputs, and if these relationships were linear or monotonic, scatter graphs were made between each input and output variable (Figures \ref{fig:xvsy0} and \ref{fig:xvsy1}). The inputs were normalised to allow for comparisons between values that could have very different ranges. To better visualise the density of the data points as well as their position, the \url{alpha} parameter was set to 0.1 in the plots. 

\begin{figure}[!ht]
\centering
\includegraphics[width=0.8 \linewidth]{images/XsVsY0}
\caption{Normalised inputs plotted against the first output variable}
\label{fig:xvsy0}
\end{figure}

\begin{figure}[!ht]
\centering
\includegraphics[width=0.8 \linewidth]{images/XsVsY1}
\caption{Normalised inputs plotted against the second output variable}
\label{fig:xvsy1}
\end{figure}

\FloatBarrier

Since the distribution of the outputs were so similiar, they were also plotted against each other in a scatter plot (Figure \ref{fig:y1vsy2}). This showed that the two variables had very similiar values, and so a regression model that applied to only one of them could likely be used for the other.

\begin{figure}[!ht]
\centering
\includegraphics[width=0.8 \linewidth]{images/Y1vsY2}
\caption{Normalised outputs against each other}
\label{fig:y1vsy2}
\end{figure}

\FloatBarrier

\part{Feature Selection}

To try and identify which features had the strongest effect on the outputs, both the Pearson and Spearman rank correlation coefficients were considered. It was noted that the Pearson correlation would give a perfect value when the two variables were linearly related, whilst the Spearman correlation (a similiar alternative, and the one used in the original paper) would give a perfect value when the variables were monotonically related. Given these factors, the Spearman rank correlation coefficient was used as a filter method.

From the scatter plots, some variables appeared to have a possible linear relationship with the outputs (for example, X7 and Y1), whilst others had a monotonic relationship (for example, X1 and Y1). Using the Scipy \emph{stats.spearmanr} method, the correlation coefficients were easily calculated, alongside a p-value, as shown in table \ref{tbl:spearman-table}. Immediately from these result we can see that X6 and X8 show little evidence of a monotonic relationship existing between them and either of the outputs. 

However, according to Schiavon et al. \cite{schiavon_lee_bauman_webster_2010}, the orientation (X6) is one of the most important predictors for cooling load (Y2). Though fewer features would improve the processing time, and removing less important features would be crucial for  eliminating noise, only X8 was removed from our feature set.

\begin{table}[!ht]
\centering
\caption{Spearman rank correlation coefficients, with p values}
\label{tbl:spearman-table}j
\begin{tabular}{||l|l|l|l||}
\hline
X    				& Y & Rho   & p    \\
\hline
1                   & 1 & 0.61  & 0.00 \\
1                   & 2 & 0.64  & 0.00 \\
2                   & 1 & -0.61 & 0.00 \\
2                   & 2 & -0.64 & 0.00 \\
3                   & 1 & 0.49  & 0.00 \\
3                   & 2 & 0.43  & 0.00 \\
4                   & 1 & -0.80 & 0.00 \\
4                   & 2 & -0.80 & 0.00 \\
5                   & 1 & 0.86  & 0.00 \\
5                   & 2 & 0.86  & 0.00 \\
6                   & 1 & 0.00  & 0.99 \\%
6                   & 2 & 0.03  & 0.51 \\
7                   & 1 & 0.35  & 0.00 \\
7                   & 2 & 0.32  & 0.00 \\
8                   & 1 & 0.09  & 0.03 \\
8                   & 2 & 0.06  & 0.14 \\
\hline
\end{tabular}
\end{table}

\part{Selecting and Training a Regression Model}

The limited size of the corpus available made the choice of algorithm especially crucial. The lack of noticable outliers in the visualistions at least suggest that the data is not of poor-quality. 




Due to the small corpus size, K-fold cross-validation was chosen to estimate the model error \cite{gron_2018}. 

\part{Performance Evaluation}

\part{Discussion}

\part{Conclusion}

\bibliographystyle{unsrt}
\bibliography{mybib}

\end{document}
